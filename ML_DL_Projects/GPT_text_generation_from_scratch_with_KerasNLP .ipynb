{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyGlOvAbK4QV"
      },
      "source": [
        "# GPT text generation from scratch with KerasNLP\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cHYhyEMK4Qc"
      },
      "source": [
        "\n",
        "\n",
        "This example requires KerasNLP. You can install it via the following command:\n",
        "`pip install keras-nlp`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSDALoTAK4Qd"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#pip install keras-nlp"
      ],
      "metadata": {
        "id": "wEEocFjbLE29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HvVTd7tfK4Qd",
        "outputId": "374fa394-690b-49ba-991e-e8ff9be6ed92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using TensorFlow backend\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import keras_nlp\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCI9dA8ZK4Qf"
      },
      "source": [
        "## Settings & hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7lE8xTwVK4Qg"
      },
      "outputs": [],
      "source": [
        "# Data\n",
        "BATCH_SIZE = 64\n",
        "SEQ_LEN = 128\n",
        "MIN_TRAINING_SEQ_LEN = 450\n",
        "\n",
        "# Model\n",
        "EMBED_DIM = 256\n",
        "FEED_FORWARD_DIM = 256\n",
        "NUM_HEADS = 3\n",
        "NUM_LAYERS = 2\n",
        "VOCAB_SIZE = 5000\n",
        "\n",
        "# Training\n",
        "EPOCHS = 6\n",
        "\n",
        "# Inference\n",
        "NUM_TOKENS_TO_GENERATE = 80"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gl9SPiyK4Qg"
      },
      "source": [
        "## Load the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LpK2FU-NK4Qh",
        "outputId": "8e2b2bcf-c2ca-40c8-acb6-1b910714edac",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\n",
            "282386239/282386239 [==============================] - 35s 0us/step\n"
          ]
        }
      ],
      "source": [
        "keras.utils.get_file(\n",
        "    origin=\"https://dldata-public.s3.us-east-2.amazonaws.com/simplebooks.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "dir = os.path.expanduser(\"~/.keras/datasets/simplebooks/\")\n",
        "\n",
        "\n",
        "raw_train_ds = (\n",
        "    tf.data.TextLineDataset(dir + \"simplebooks-92-raw/train.txt\")\n",
        "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .shuffle(buffer_size=256)\n",
        ")\n",
        "\n",
        "raw_val_ds = (\n",
        "    tf.data.TextLineDataset(dir + \"simplebooks-92-raw/valid.txt\")\n",
        "    .filter(lambda x: tf.strings.length(x) > MIN_TRAINING_SEQ_LEN)\n",
        "    .batch(BATCH_SIZE)\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpjWJ76UK4Qi"
      },
      "source": [
        "## Train the tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZSZkHIJfK4Qi"
      },
      "outputs": [],
      "source": [
        "# Train\n",
        "vocab = keras_nlp.tokenizers.compute_word_piece_vocabulary(\n",
        "    raw_train_ds,\n",
        "    vocabulary_size=VOCAB_SIZE,\n",
        "    lowercase=True,\n",
        "    reserved_tokens=[\"[PAD]\", \"[UNK]\", \"[BOS]\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QqKOzqvuK4Qj"
      },
      "source": [
        "## Load tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NO1n4SpAK4Qj"
      },
      "outputs": [],
      "source": [
        "tokenizer = keras_nlp.tokenizers.WordPieceTokenizer(\n",
        "    vocabulary=vocab,\n",
        "    sequence_length=SEQ_LEN,\n",
        "    lowercase=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqa7wTP9K4Qj"
      },
      "source": [
        "## Tokenize data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btBGxjjdK4Qj"
      },
      "outputs": [],
      "source": [
        "# packer adds\n",
        "start_packer = keras_nlp.layers.StartEndPacker(\n",
        "    sequence_length=SEQ_LEN,\n",
        "    start_value=tokenizer.token_to_id(\"[BOS]\"),\n",
        ")\n",
        "\n",
        "\n",
        "def preprocess(inputs):\n",
        "    outputs = tokenizer(inputs)\n",
        "    features = start_packer(outputs)\n",
        "    labels = outputs\n",
        "    return features, labels\n",
        "\n",
        "\n",
        "# Tokenize and split\n",
        "train_ds = raw_train_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n",
        "    tf.data.AUTOTUNE\n",
        ")\n",
        "val_ds = raw_val_ds.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE).prefetch(\n",
        "    tf.data.AUTOTUNE\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lw_npwfdK4Qk"
      },
      "source": [
        "## Build the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BEnanJZQK4Qk"
      },
      "outputs": [],
      "source": [
        "inputs = keras.layers.Input(shape=(None,), dtype=tf.int32)\n",
        "# Embedding.\n",
        "embedding_layer = keras_nlp.layers.TokenAndPositionEmbedding(\n",
        "    vocabulary_size=VOCAB_SIZE,\n",
        "    sequence_length=SEQ_LEN,\n",
        "    embedding_dim=EMBED_DIM,\n",
        "    mask_zero=True,\n",
        ")\n",
        "x = embedding_layer(inputs)\n",
        "# Transformer decoders.\n",
        "for _ in range(NUM_LAYERS):\n",
        "    decoder_layer = keras_nlp.layers.TransformerDecoder(\n",
        "        num_heads=NUM_HEADS,\n",
        "        intermediate_dim=FEED_FORWARD_DIM,\n",
        "    )\n",
        "    x = decoder_layer(x)\n",
        "# Output.\n",
        "outputs = keras.layers.Dense(VOCAB_SIZE)(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "perplexity = keras_nlp.metrics.Perplexity(from_logits=True, mask_token_id=0)\n",
        "model.compile(optimizer=\"adam\", loss=loss_fn, metrics=[perplexity])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mpZg-88gK4Ql",
        "outputId": "d6495ec5-8287-4e27-a1c2-1bf56673b809",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " token_and_position_embeddi  (None, None, 256)         1312768   \n",
            " ng (TokenAndPositionEmbedd                                      \n",
            " ing)                                                            \n",
            "                                                                 \n",
            " transformer_decoder (Trans  (None, None, 256)         394749    \n",
            " formerDecoder)                                                  \n",
            "                                                                 \n",
            " transformer_decoder_1 (Tra  (None, None, 256)         394749    \n",
            " nsformerDecoder)                                                \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, None, 5000)        1285000   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3387266 (12.92 MB)\n",
            "Trainable params: 3387266 (12.92 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aHnb2T3xK4Ql"
      },
      "source": [
        "## Training\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8UjQcmZ5K4Ql",
        "outputId": "8fe77346-b3aa-405a-d61a-54f98274224a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "3169/3169 - 245s - loss: 3.9990 - perplexity: 54.7545 - val_loss: 3.9973 - val_perplexity: 54.9958 - 245s/epoch - 77ms/step\n",
            "Epoch 2/2\n",
            "3169/3169 - 242s - loss: 3.9141 - perplexity: 50.2963 - val_loss: 3.9260 - val_perplexity: 51.1246 - 242s/epoch - 76ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c2b30491210>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "model.fit(train_ds, validation_data=val_ds, verbose=2, epochs=2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3FrJTzvK4Ql"
      },
      "source": [
        "## Inference\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnSsjCsiK4Qm",
        "outputId": "743b4c8a-36d6-4c9d-c027-ad447beccdc1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1, 128), dtype=int32, numpy=\n",
              "array([[2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
              "      dtype=int32)>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "prompt_tokens = start_packer(tokenizer([\"\"]))\n",
        "prompt_tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YZyep0IqK4Qm"
      },
      "outputs": [],
      "source": [
        "\n",
        "def next(prompt, cache, index):\n",
        "    logits = model(prompt)[:, index - 1, :]\n",
        "    hidden_states = None\n",
        "    return logits, hidden_states, cache\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HlNSweR_K4Qn"
      },
      "source": [
        "### Greedy search\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "daLEMsnBK4Qn",
        "outputId": "e6870373-a2da-46ff-f0e4-88108c498c3c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Greedy search generated text: \n",
            "[b'[BOS] the next day the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the king of the']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sampler = keras_nlp.samplers.GreedySampler()\n",
        "output_tokens = sampler(\n",
        "    next=next,\n",
        "    prompt=prompt_tokens,\n",
        "    index=1,\n",
        ")\n",
        "txt = tokenizer.detokenize(output_tokens)\n",
        "print(f\"Greedy search generated text: \\n{txt}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2BVkY6ghK4Qn"
      },
      "source": [
        "### Beam search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KDKSgvFwK4Qn",
        "outputId": "0f5f1e42-8fac-4d4c-8c98-77f3b53ff4fb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam search generated text: \n",
            "[b'[BOS] \" well , \" he said , \" i don \\' t want to tell you , \" he said , \" i don \\' t know what to do . i \\' m going to tell you , but i \\' m going to tell you . i \\' m going to tell you , but i \\' m going to tell you . i \\' m going to tell you , but i \\' m going to tell you . i \\' m going to tell you , but i \\' m going to tell you . i \\' m going to tell you . i \\' m going to tell you , but i \\' m going to tell you . i \\' m']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sampler = keras_nlp.samplers.BeamSampler(num_beams=10)\n",
        "output_tokens = sampler(\n",
        "    next=next,\n",
        "    prompt=prompt_tokens,\n",
        "    index=1,\n",
        ")\n",
        "txt = tokenizer.detokenize(output_tokens)\n",
        "print(f\"Beam search generated text: \\n{txt}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "whi65TOrK4Qo"
      },
      "source": [
        "### Random search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S15ceawzK4Qo",
        "outputId": "2e2d8846-6b16-4c5b-960d-2fc3556fbff3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Random search generated text: \n",
            "[b'[BOS] and at these days , because it kept a sort of men all new pity to waste flow of the weeds which batzes nowlands , for instance , was harder than it did me i think ? \" or \" are quite tiny gerblanger and wants no finerumb and good , and i can never think that we \\' d been too poor is our children \\' s self - thought . our herod is quite enough to trade us , need to let our use lie so doing a beautiful day , even if ill we would win them with this day . i will forget him , and i \\' m']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sampler = keras_nlp.samplers.RandomSampler()\n",
        "output_tokens = sampler(\n",
        "    next=next,\n",
        "    prompt=prompt_tokens,\n",
        "    index=1,\n",
        ")\n",
        "txt = tokenizer.detokenize(output_tokens)\n",
        "print(f\"Random search generated text: \\n{txt}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vrCb0mdHK4Qo"
      },
      "source": [
        "### Top-K search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZihTPDc6K4Qo",
        "outputId": "2ce5c288-933d-419f-9362-1f7c10227844",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-K search generated text: \n",
            "[b'[BOS] \" i know , \" said he , \" that \\' s the way i have to go out . i must go with you to do the same , but i must tell you . i know the story i tell you that you must tell me the story to tell you . i told that you must do something that you have asked me if you had been a great many people that have been born to - day , and you would be sure to tell what they would have to have you . it \\' s true , and the old lady was afraid that the lady was in her heart of the world , and you must remember the truth , for']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sampler = keras_nlp.samplers.TopKSampler(k=10)\n",
        "output_tokens = sampler(\n",
        "    next=next,\n",
        "    prompt=prompt_tokens,\n",
        "    index=1,\n",
        ")\n",
        "txt = tokenizer.detokenize(output_tokens)\n",
        "print(f\"Top-K search generated text: \\n{txt}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EI_nuFTZK4Qp"
      },
      "source": [
        "### Top-P search\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8nB_LOyHK4Qp",
        "outputId": "879c38ca-6b46-41ea-d049-a27f0bd2f82a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-P search generated text: \n",
            "[b'[BOS] the big man had , and he was a great many years old , but he had been the man of the country , and had lived in the wilderness , who had become his wife and childhood . but the king had never ceased to hope to speak to him , and , as he had his mind to go , he would never find a great way to him . he had only to live with him , and when he saw the little old man on his side , he was sure that the man had to come to him . but he did not find his daughter to her . [PAD] , as she spoke , he said , \" i can']\n",
            "\n"
          ]
        }
      ],
      "source": [
        "sampler = keras_nlp.samplers.TopPSampler(p=0.5)\n",
        "output_tokens = sampler(\n",
        "    next=next,\n",
        "    prompt=prompt_tokens,\n",
        "    index=1,\n",
        ")\n",
        "txt = tokenizer.detokenize(output_tokens)\n",
        "print(f\"Top-P search generated text: \\n{txt}\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2PukP2ySK4Qp"
      },
      "source": [
        "### Using callbacks for text generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_yMOBeICK4Qv",
        "outputId": "28a54de5-d2a8-4ccc-ac9a-3b36eb51c9cd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n",
            "Top-K search generated text: \n",
            "[b'[BOS] then , when the two boys had taken , they were very hungry , and they ate and ate , and then their breakfast was eaten . it was not that of course , because they wanted to be happy to be hungry . they thought it was very good for them ! so that they could not bear very much of it . so they ate a little supper , and they ate it all night . then , as the boys ate , they went away . they did not know what to eat , but when the little dog came , it came out of his father , he went to bed . [PAD] , in a short time , when he found a']\n",
            "\n",
            "1/1 - 11s - loss: 3.7616 - perplexity: 43.1453 - 11s/epoch - 11s/step\n",
            "Epoch 2/2\n",
            "Top-K search generated text: \n",
            "[b'[BOS] after they reached the place where they had been , and that in all the afternoon they were at the beginning of the year , and there was little more than one of the other . they were to go in a little distance , and , for the most part in their way of the world , they would be to be so far that they could not be so easy . they were in a way to make the best of the landlord , and that was why should they be called on to the king oriana , for he was the king of that place , wherefore he had told them what was the best way of making them .']\n",
            "\n",
            "1/1 - 15s - loss: 3.8985 - perplexity: 49.7041 - 15s/epoch - 15s/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7c2b302c8b80>"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "\n",
        "class TopKTextGenerator(keras.callbacks.Callback):\n",
        "    \"\"\"A callback to generate text from a trained model using top-k.\"\"\"\n",
        "\n",
        "    def __init__(self, k):\n",
        "        self.sampler = keras_nlp.samplers.TopKSampler(k)\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        output_tokens = self.sampler(\n",
        "            next=next,\n",
        "            prompt=prompt_tokens,\n",
        "            index=1,\n",
        "        )\n",
        "        txt = tokenizer.detokenize(output_tokens)\n",
        "        print(f\"Top-K search generated text: \\n{txt}\\n\")\n",
        "\n",
        "\n",
        "text_generation_callback = TopKTextGenerator(k=10)\n",
        "model.fit(train_ds.take(1), verbose=2, epochs=2, callbacks=[text_generation_callback])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}